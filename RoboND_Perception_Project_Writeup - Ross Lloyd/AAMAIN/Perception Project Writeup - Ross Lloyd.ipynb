{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Perception Pick & Place\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Exercise 1, 2 and 3 pipeline implemented\n",
    "\n",
    "### 1. Complete Exercise 1 steps. Pipeline for filtering and RANSAC plane fitting implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert ROS msg to PCL data\n",
    "\tmsg_to_pcl =  ros_to_pcl(pcl_msg)   \n",
    "# TODO: Statistical Outlier Filtering\n",
    "\t# We start by creating a filter object: \n",
    "\toutlier_filter = msg_to_pcl.make_statistical_outlier_filter()\n",
    "\t# Set the number of neighboring points to analyze for any given point. The larger the value of k\n",
    "    # and so the more surrounding points considered by the function, the less effective the filter was\n",
    "\toutlier_filter.set_mean_k(5)\n",
    "\t# Set threshold scale factor. Here a smaller threshold helped 'clean up' the final generated clusters\n",
    "    # and give them a more defined edge.\n",
    "\tx = 0.000000001\n",
    "\t# Any point with a mean distance larger than global (mean distance+x*std_dev) will be considered outlier\n",
    "\toutlier_filter.set_std_dev_mul_thresh(x)\n",
    "\t# Finally call the filter function for magic\n",
    "\tmsg_to_pcl = outlier_filter.filter()\n",
    "\n",
    "\n",
    "# TODO: Voxel Grid Downsampling\n",
    "\tvox = msg_to_pcl.make_voxel_grid_filter()\n",
    "    #The leaf size dictates the volume of each voxel. I set the size to the smallest allowable as this made\n",
    "    #my pipeline robust over all three worlds.\n",
    "\tLEAF_SIZE = 0.006\n",
    "\tvox.set_leaf_size(LEAF_SIZE, LEAF_SIZE, LEAF_SIZE)\n",
    "\tcloud_filtered = vox.filter()\n",
    "\n",
    "\n",
    "# TODO: PassThrough Filter Z. This is the standard passthrough given in the class and works in the \n",
    "# 'vertical' axis, allowing us to remove extraneous part of the table from consideration.\n",
    "\tpassthroughZ = cloud_filtered.make_passthrough_filter()\n",
    "\tfilter_axis = 'z'\n",
    "\tpassthroughZ.set_filter_field_name(filter_axis)\n",
    "    #The numbers for min and max allowed me to focus on the objects better without potentially excluding\n",
    "    #any cloud points belonging to the objects. However this did lead to some voxels remaining in view\n",
    "    #from the thin vertical edge of the table.\n",
    "\taxis_min = 0.598\n",
    "\taxis_max = 1.1\n",
    "\tpassthroughZ.set_filter_limits(axis_min, axis_max)\n",
    "\tcloud_filtered = passthroughZ.filter()\n",
    "    \n",
    "\t# Passthrough filter X to remove appearance of bin edges\n",
    "    # Adding the x axis filter allowed me to remove the table edge excess mentioned above as well as exclude the\n",
    "    #visible edges of the dropboxes.\n",
    "\tpassthroughX = cloud_filtered.make_passthrough_filter()\n",
    "\tfilter_axis = 'x'\n",
    "\tpassthroughX.set_filter_field_name(filter_axis)\n",
    "\taxis_min = 0.35\n",
    "\taxis_max = 3.6\n",
    "\tpassthroughX.set_filter_limits(axis_min, axis_max)\n",
    "\tcloud_filtered = passthroughX.filter()\n",
    "    \n",
    "# TODO: RANSAC Plane Segmentation\n",
    "\tseg = cloud_filtered.make_segmenter()\n",
    "\t#Use the model for the plane to exclude the surface of the table.\n",
    "    seg.set_model_type(pcl.SACMODEL_PLANE)\n",
    "\tseg.set_method_type(pcl.SAC_RANSAC)\n",
    "    #Setting max distance fairly small to help prevent losing points belonging to other objects.\n",
    "\tmax_distance = 0.0075\n",
    "\tseg.set_distance_threshold(max_distance)\n",
    "    \n",
    "# TODO: Extract inliers and outliers. Here we separate the objects from the table surface and create two new \n",
    "# point clouds, cloud_table and cloud_objects:\n",
    "\tinliers, coefficients = seg.segment()\n",
    "\textracted_inliers = cloud_filtered.extract(inliers, negative=False)\n",
    "\textracted_outliers = cloud_filtered.extract(inliers, negative=True)\n",
    "\tcloud_table = extracted_inliers\n",
    "\tcloud_objects = extracted_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration of effects of point cloud filters and their parameter settings:\n",
    "\n",
    "### Outlier Filter\n",
    "\n",
    "Here is a shot without any outlier (noise) filtration. The perception pipeline is unable to separate out clusters due to the extensive overlap of meaningful point cloud data with noise:\n",
    "\n",
    "![DH_Board](../images/no-outlier-filter.png)\n",
    "\n",
    "Here the filter is active, but the value of k, which sets how many surrounding points to take into account when establishing which points are unwanted, is too high:\n",
    "\n",
    "![DH_Board](../images/with-outlier-filt-k-too-high100.png)\n",
    "\n",
    "Now, using a value of k=5, we can see that the image is much cleaner and the pipeline can identify the objects:\n",
    "\n",
    "![DH_Board](../images/With-outlier-x-0-01-k5.png)\n",
    "\n",
    "Increasing the value of x (the tolerance) further cleans up the cloud and produces well defined clusters in conjunction with the rest of the pipeline:\n",
    "\n",
    "![DH_Board](../images/with-outlier-k-hi-x-hi-clean.png)\n",
    "\n",
    "### Voxel Grid Filter\n",
    "\n",
    "The purpose of the voxel grid filter is to reduce the number of points to be processed by the pipeline. To this end, changing the value of the leaf size affects the density of the point cloud we want to use in our pipeline:\n",
    "\n",
    "Leaf size = 0.01\n",
    "\n",
    "![DH_Board](../images/Voxel-Leaf_Size-0-01.png)\n",
    "\n",
    "Leaf size = 0.006. Note that the pipeline for this world has identified all the objects correctly, however I chose the finer leaf size of 0.006 as it made detection for all worlds more robust:\n",
    "\n",
    "![DH_Board](../images/Voxel-Leaf-Size-0-006.png)\n",
    "\n",
    "### Passthrough filters:\n",
    "\n",
    "The aim of the passthrough filter is to eliminate parts of the pointcloud from consideration, in this case the legs and egde of the table, as well as the dropboxes at the far edges of the image. It is critical to set the axes values correctly so as not to exclude any wanted points or include any extraneous data. \n",
    "\n",
    "Here is what it looks like when the values are set wrongly. Some of the objects are cut off, like a bad photographer cutting people's heads out of a picture. We can also see too much of the table edge (black pixels):\n",
    "\n",
    "![DH_Board](../images/passthrough-wrong.png)\n",
    "\n",
    "Even with the z passthrough, we can still see the edges of the dropboxes. Additionally, as we do not want to set the z passthrough so close to the surface of the table that we begin to exclude point data that belongs to the bottom of our objects, we can still see some points from the table edge (black pixels). \n",
    "\n",
    "![DH_Board](../images/passthrough-only-z-no-x.png)\n",
    "\n",
    "These can both be eliminated with a passthrough filter on the x axis (running left to right looking at the table and passing between the objects and the robot). \n",
    "\n",
    "![DH_Board](../images/passthrough-xandz.png)\n",
    "\n",
    "## Plane Segmentation\n",
    "\n",
    "Next we apply plane segmentation, and can use the PCL/Table topic in RVIZ to view the output just for the table surface:\n",
    "\n",
    "![DH_Board](../images/plane-seg-table-only.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete Exercise 2 steps: Pipeline including clustering for segmentation implemented.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Euclidean Clustering\n",
    "\twhite_cloud = XYZRGB_to_XYZ(cloud_objects)\n",
    "# TODO: Create Cluster-Mask Point Cloud to visualize each cluster separately\n",
    "\ttree = white_cloud.make_kdtree()\n",
    "\tec = white_cloud.make_EuclideanClusterExtraction()\n",
    "    \n",
    "# TODO: Convert PCL data to ROS messages\n",
    "# Cluster tolerance is important - it helps the filter distinguish between two objects that may be close together:\n",
    "\tec.set_ClusterTolerance(0.0500)\n",
    "    #Setting Min cluster size correctly helps identifying smaller objects such as the glue \n",
    "\tec.set_MinClusterSize(50)\n",
    "\tec.set_MaxClusterSize(5000)\n",
    "\tec.set_SearchMethod(tree)\n",
    "\tcluster_indices = ec.Extract()\n",
    "    \n",
    "    #Assign a color corresponding to each segmented object in scene\n",
    "\tcluster_color = get_color_list(len(cluster_indices))\n",
    "\tcolor_cluster_point_list = []\n",
    "\n",
    "\tfor j, indices in enumerate(cluster_indices):\n",
    "\t\tfor i, indice in enumerate(indices):\n",
    "\t\t\tcolor_cluster_point_list.append([white_cloud[indice][0], \n",
    "\t\t\t\t\t\t\t\t\t\twhite_cloud[indice][1], \n",
    "\t\t\t\t\t\t\t\t\t\twhite_cloud[indice][2],\n",
    "\t\t\t\t\t\t\t\t\t\trgb_to_float(cluster_color[j])])\n",
    "\n",
    "    #Create new cloud containing all clusters, each with unique color\n",
    "\tcluster_cloud = pcl.PointCloud_PointXYZRGB()\n",
    "\tcluster_cloud.from_list(color_cluster_point_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the correct sensitivities for the clustering function, we are able to identify small enough clusters for objects like the glue, and large enough clusters for objects such as the biscuits. \n",
    "\n",
    "![DH_Board](../images/euclidian-clusters.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform object recognition on these objects and assign them labels (markers in RViz).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Classify the clusters! (loop through each detected cluster one at a time)\n",
    "\tdetected_objects_labels = []\n",
    "\tdetected_objects = []\n",
    "\tfor index, pts_list in enumerate(cluster_indices):    \n",
    "        # Grab the points for the cluster\n",
    "\t\tpcl_cluster = cloud_objects.extract(pts_list)\n",
    "\t\tros_cluster = pcl_to_ros(pcl_cluster)\n",
    "\t\t# Compute the associated feature vector. Here we call the histogram functions created earlier in the project\n",
    "        #and which produce arrays of hsv PCL data for later fitting and object recognition\n",
    "\t\tchists = compute_color_histograms(ros_cluster, using_hsv=True)#ros_pcl_cluster, using_hsv=False)\n",
    "\t\tnormals = get_normals(ros_cluster)#ros_pcl_cluster) as changed above    \n",
    "\t\tnhists = compute_normal_histograms(normals)\n",
    "\t\tfeature = np.concatenate((chists, nhists))\n",
    "\t\t\n",
    "        # Make the prediction. \n",
    "\t\tprediction = clf.predict(scaler.transform(feature.reshape(1,-1)))\n",
    "\t\tlabel = encoder.inverse_transform(prediction)[0]\n",
    "\t\tdetected_objects_labels.append(label)\n",
    "\t\t\n",
    "        # Publish a label into RViz\n",
    "\t\tlabel_pos = list(white_cloud[pts_list[0]])\n",
    "\t\tlabel_pos[2] += .4\n",
    "\t\tobject_markers_pub.publish(make_label(label,label_pos, index))\n",
    "\t\t\n",
    "\t\t# Add the detected object to the list of detected objects. Uses DetectedObject.msg, a \n",
    "\t\t# two line file that defines the msg fields\n",
    "\t\tdo = DetectedObject()\n",
    "\t\t#Populate the msg fields\n",
    "\t\tdo.label = label\n",
    "\t\t#Populate with cluster array\n",
    "\t\tdo.cloud = ros_cluster\n",
    "\t\t#this is the detected objects list that we later compare the pick list to, outputs\n",
    "\t\t# format e.g. ['biscuits', 'soap'. 'soap2']\n",
    "\t\tdetected_objects.append(do)\n",
    "\t#print('test = ', do.label)\n",
    "    \n",
    "\trospy.loginfo('Detected {} objects: {}'.format(len(detected_objects_labels), detected_objects_labels))\n",
    "    # Publish the list of detected objects\n",
    "\tdetected_objects_pub.publish(detected_objects)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Note that the data used for the classifier, such as clf and scaler, come from the clustering node, which\n",
    "    #parses the model.sav file created by the training process:\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # TODO: ROS node initialization\n",
    "\trospy.init_node('clustering', anonymous=True)\n",
    "    # TODO: Create Subscribers\n",
    "\tpcl_sub = rospy.Subscriber(\"/pr2/world/points\", pc2.PointCloud2, pcl_callback, queue_size=1)\n",
    "    # TODO: Create Publishers\n",
    "\tdetected_objects_pub = rospy.Publisher(\"/detected_objects\", DetectedObjectsArray, queue_size=1)\n",
    "\tobject_markers_pub = rospy.Publisher(\"/object_markers\", Marker, queue_size=1)\n",
    "\tpcl_objects_pub = rospy.Publisher(\"/pcl_objects\", PointCloud2, queue_size=1)\n",
    "\tpcl_table_pub = rospy.Publisher(\"/pcl_table\", PointCloud2, queue_size=1)\n",
    "\tpcl_cluster_pub = rospy.Publisher(\"/pcl_cluster\", PointCloud2, queue_size=1)    \n",
    "    # TODO: Load Model From disk\n",
    "\tmodel = pickle.load(open('model.sav', 'rb'))\n",
    "\tclf = model['classifier']\n",
    "\tencoder = LabelEncoder()\n",
    "\tencoder.classes_ = model['classes']\n",
    "\tscaler = model['scaler']\n",
    "    # Initialize color_list\n",
    "\tget_color_list.color_list = []\n",
    "\n",
    "    # TODO: Spin while node is not shutdown\n",
    "\twhile not rospy.is_shutdown():\n",
    "\t\trospy.spin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screenshots of accurately identified objects for all three standard test worlds:\n",
    "\n",
    "World 1, 100% identified (clusters shown)\n",
    "\n",
    "![DH_Board](../images/World1_Clusters.png)\n",
    "\n",
    "World 1, 100% identified (objects shown)\n",
    "\n",
    "![DH_Board](../images/World1_Objects.png)\n",
    "\n",
    "World 2, 100% identified (clusters shown)\n",
    "\n",
    "![DH_Board](../images/World2_clusters.png)\n",
    "\n",
    "World 2, 100% identified (objects shown)\n",
    "\n",
    "![DH_Board](../images/world2.png)\n",
    "\n",
    "World 3, 100% identified (clusters shown)\n",
    "\n",
    "![DH_Board](../images/World3_cluster.png)\n",
    "\n",
    "World 3, 100% identified (objects shown)\n",
    "\n",
    "![DH_Board](../images/World3Objects.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Complete Exercise 3 Steps.  Features extracted and SVM trained.  Object recognition implemented.\n",
    "\n",
    "Below is the code used to generate the color histograms from the point cloud. I decided to use the voxel downsampling to make the clouds used in processing smaller. This allowed me to do 1000 pose-per-object training runs without it taking a prohibitive amount of time. Accuracy in the confusion matrix test was improved by 3 percentage points by doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_color_histograms(cloud, using_hsv=True):\n",
    "    #apply voxel grid downsampling to improve speed of data acquisition:\n",
    "    cloud = ros_to_pcl(cloud)\n",
    "    vox = cloud.make_voxel_grid_filter()\n",
    "    LEAF_SIZE = 0.006\n",
    "    vox.set_leaf_size(LEAF_SIZE, LEAF_SIZE, LEAF_SIZE)\n",
    "    cloud = vox.filter()\n",
    "    cloud = pcl_to_ros(cloud)\n",
    "\n",
    "    # Compute histograms for the clusters\n",
    "    point_colors_list = []\n",
    "\n",
    "    # Step through each point in the point cloud\n",
    "    for point in pc2.read_points(cloud, skip_nans=True):\n",
    "        rgb_list = float_to_rgb(point[3])\n",
    "        if using_hsv:\n",
    "            point_colors_list.append(rgb_to_hsv(rgb_list) * 255)\n",
    "        else:\n",
    "            point_colors_list.append(rgb_list)\n",
    "\n",
    "    # Populate lists with color values\n",
    "    channel_1_vals = []\n",
    "    channel_2_vals = []\n",
    "    channel_3_vals = []\n",
    "\n",
    "    for color in point_colors_list:\n",
    "        channel_1_vals.append(color[0])\n",
    "        channel_2_vals.append(color[1])\n",
    "        channel_3_vals.append(color[2])\n",
    "    \n",
    "\n",
    "    # TODO: Compute histograms\n",
    "\tnbins = 32\n",
    "\tbins_range = (0, 256)\n",
    "\tch_1 = np.histogram(channel_1_vals, bins=nbins, range=bins_range)\n",
    "\tch_2 = np.histogram(channel_2_vals, bins=nbins, range=bins_range)\n",
    "\tch_3 = np.histogram(channel_3_vals, bins=nbins, range=bins_range)\n",
    "\n",
    " #    # TODO: Concatenate and normalize the histograms\n",
    "    ch_features = np.concatenate((ch_1[0], ch_2[0], ch_3[0])).astype(np.float64)    \n",
    "  \n",
    "    # Replace normed_features with your feature vector\n",
    "    normed_features = ch_features / np.sum(ch_features) \n",
    "    return normed_features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the normal histograms I was not able to use the voxel downsampling due to data problems I was unable to solve. However the gains from using it with the colour section above meant that data acquisition was still much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normal_histograms(normal_cloud):\n",
    "\n",
    "    # Compute histograms for the clusters\n",
    "    point_colors_list = []\n",
    "    norm_x_vals = []\n",
    "    norm_y_vals = []\n",
    "    norm_z_vals = []\n",
    "\n",
    "    #normal_cloud changed to normal_cloud_vox\n",
    "    for norm_component in pc2.read_points(normal_cloud,\n",
    "                                          field_names = ('normal_x', 'normal_y', 'normal_z'),\n",
    "                                          skip_nans=True):\n",
    "        norm_x_vals.append(norm_component[0])\n",
    "        norm_y_vals.append(norm_component[1])\n",
    "        norm_z_vals.append(norm_component[2])\n",
    "       \n",
    "\n",
    "\n",
    "    # TODO: Compute histograms of normal values (just like with color)\n",
    "\tnbins = 32\n",
    "\t#Note this range can be -1, 1 or 0, 256. -1, 1 seems to work better with bayes\n",
    "\tbins_range = (0, 256)\n",
    "\tx = np.histogram(norm_x_vals, bins=nbins, range=bins_range)\n",
    "\ty = np.histogram(norm_y_vals, bins=nbins, range=bins_range)\n",
    "\tz = np.histogram(norm_z_vals, bins=nbins, range=bins_range)\n",
    "\n",
    "    # TODO: Concatenate and normalize the histograms\n",
    "    xyz_features = np.concatenate((x[0], y[0], z[0])).astype(np.float64)\n",
    "\n",
    "    # Replace normed_features with your feature vector\n",
    "    normed_features = xyz_features / np.sum(xyz_features)\n",
    "\n",
    "    return normed_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the svm\n",
    "\n",
    "I experimented with a number of classifiers, including Naive Bayes, SVM(rbf) and SVM(linear). Bayes is reportedly a good choice for extensively overlapping objects as it performs better than SVM's under those conditions, however in my tests with the provided worlds, by far the best performance was with linear SVM and modest C values. Rbf could be somewhat hit or miss, with the possibility of over-fitting being the cause.\n",
    "\n",
    "I was very happy with the performance of the linear classifier which allowed me to identify all objects in all worlds. It is likely that the high number of training poses contributed to this, though even at around 100 poses per object the classifier was still very effective. Identifying the glue in world 3 could be problematic on lower training counts however. The classifier gave an accuracy of 97% as illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Accuracy in terminal](../images/accuracy_term.png)\n",
    "![Confusion Matrix](../images/confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the robust performance of the classifier, it was possible to cause it to fail by moving objects to be behind and very close to one another. Though it was possible to offset some of this by adjusting the cluster tolerance, there came a point where it was very hard for the pipeline to separate the objects. I had hoped that Naive Bayes would help in this situation however it did not. Leveraging its reportedly superior performance in overlapping situations is something that would be worth investigating to make the pipeline even more powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick and Place Setup\n",
    "\n",
    "#### 1. For all three tabletop setups ('test_world'), perform object recognition, then read in respective pick list (pick_list_*.yaml). Next construct the messages that would comprise a valid 'PickPlace' request output them to yaml format.\n",
    "\n",
    "The final step of the project involved outputting the centroids and correct arm / dropbox selection to a yaml file. The three output files are included with this repo. Here is the code needed to output the file (please see included code comments). Note that due to time constraints I chose not to pursue the actual pick and place step or any of the challenge worlds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr2_mover(object_list):\n",
    "# TODO: Initialize variables\n",
    "\tdict_list = []\n",
    "\tlabels = []\n",
    "\tcentroids = [] # to be list of tuples (x, y, z)\n",
    "# TODO: Get/Read parameters\n",
    "\tobject_list_param = rospy.get_param('/object_list')\n",
    "\tdropbox_param = rospy.get_param('/dropbox')\n",
    "    \n",
    "# TODO: Loop through the pick list\n",
    "# TODO: Parse parameters into individual variables\n",
    "\tobj_list_name_list = []\n",
    "\tobj_list_group_list = []\n",
    "\n",
    "\tfor i in object_list_param:\n",
    "\t\tobject_name = String()\n",
    "\t\tobject_group = String()\n",
    "\t\tobject_name.data = i['name']\n",
    "\t\tobject_group.data = i['group']\n",
    "\n",
    "\n",
    "# TODO: Assign the arm to be used for pick_place\n",
    "\t\tarm_name = String()\n",
    "    #These steps parse the pick list and look for the group colour, which indicates\n",
    "    #the correct arm to use:\n",
    "\t\tif i['group'] == 'red':\n",
    "\t\t\tarm_name.data = 'left'\n",
    "\t\telif i['group'] == 'green':\n",
    "\t\t\tarm_name.data = 'right'\n",
    "        \n",
    "# TODO: Get the PointCloud for a given object and obtain it's centroid\n",
    "\t\t#for every object we detected ....\n",
    "        for obj in object_list:\n",
    "            #and if the label of the detected object matches our object in the pick list...\n",
    "\t\t\tif obj.label == object_name.data:\n",
    "\t\t\t\t#set the pick pose geometry message with the correct coords for the centroid:\n",
    "\t\t\t\tpick_pose = Pose() \n",
    "\t\t\t\tlabels.append(obj.label)\n",
    "\t\t\t\tpoints_arr = ros_to_pcl(obj.cloud).to_array()\n",
    "\n",
    "\t\t\t\tobj_centroid = np.mean(points_arr, axis=0)[:3]\n",
    "\t\t\t\tcentroids.append(obj_centroid)\t\n",
    "            #remember to set asscalar in order to provide the expected native python float:\n",
    "\t\t\t\tpick_pose.position.x = np.asscalar(obj_centroid[0])\n",
    "\t\t\t\tpick_pose.position.y = np.asscalar(obj_centroid[1])\n",
    "\t\t\t\tpick_pose.position.z = np.asscalar(obj_centroid[2])\n",
    "# TODO: Create 'place_pose' for the object\n",
    "\t\tfor k in dropbox_param:\n",
    "\t\t\tif k['group'] == object_group.data:\n",
    "\t\t\t\tplace_pose = Pose()\n",
    "\t\t\t\t#Again we create the ROS geometry message and populate it\n",
    "\t\t\t\tplace_pose.position.x = (k['position'][0])\n",
    "\t\t\t\tplace_pose.position.y = (k['position'][1])\n",
    "\t\t\t\tplace_pose.position.z = (k['position'][2])\n",
    "\n",
    "\t\t#Create and correctly set the datatype for test_scene_num\n",
    "        test_scene_num = Int32()\n",
    "\t\ttest_scene_num.data = 2\n",
    "        #create the yaml dict\n",
    "\t\tyaml_dict = make_yaml_dict(test_scene_num, arm_name, object_name, pick_pose, place_pose)\n",
    "\n",
    "# TODO: Create a list of dictionaries (made with make_yaml_dict()) for later output to yaml forma\n",
    "\t\tdict_list.append(yaml_dict)\t\t\n",
    "\n",
    "# TODO: Rotate PR2 in place to capture side tables for the collision map\n",
    "    # Not required, I am only focusing on outputting the yaml file\n",
    "\n",
    "        # Wait for 'pick_place_routine' service to come up\n",
    "\t\trospy.wait_for_service('pick_place_routine')\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tpick_place_routine = rospy.ServiceProxy('pick_place_routine', PickPlace)\n",
    "\n",
    "\t #        # TODO: Insert your message variables to be sent as a service request\n",
    "\t\t\tresp = pick_place_routine(test_scene_num, arm_name, object_name, pick_pose, place_pose)\n",
    "\n",
    "\t\t \tprint (\"Response: \",resp.success)\n",
    "\n",
    "\t\texcept rospy.ServiceException, e:\n",
    "\t\t \tprint \"Service call failed: %s\"%e\n",
    "\n",
    "# TODO: Output your request parameters into output yaml file\n",
    "#note the variable for test world is set appropriately each time\n",
    "\tyaml_filename = 'output2.yaml'\n",
    "\n",
    "\tsend_to_yaml(yaml_filename, dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My pipeline was able to filter out noise, downsample the pointcloud, eliminate unwanted regions of the scene, conduct plane segmentation for the table and separate out the objects for identification, create separate, clear clusters for each object and finally, through a properly trained SVM, identify the correct object and create labels that were displayed in RVIZ.\n",
    "\n",
    "If I were to take this project further, I would like to work on passing the yaml output to the pick place server so that it can carry out the pick place routine, however I did not have enough time to complete this part. I would also like to improve the capability of the pipeline so that it can handle overlapping and closely placed objects. I would also like to apply it to real world camera data, which would require the additional step of camera calibration and considering lighting conditions (depending on sensor type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
